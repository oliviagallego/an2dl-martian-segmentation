{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10108948,"sourceType":"datasetVersion","datasetId":6194117}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1GGuSiYQGuBSLNvYoCB93uWOTrJuXpwmC","timestamp":1733590126363},{"file_id":"https://github.com/giulio8/ANNDL-HW2/blob/main/training_after_split.ipynb","timestamp":1733586544354}]}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"eb2Nqybr7r9m","cell_type":"markdown","source":["## ðŸŒ Connect Colab to Google Drive"],"metadata":{"id":"eb2Nqybr7r9m"}},{"id":"8lGg13S_7tv4","cell_type":"code","source":["try:\n","\n","  import google.colab\n","\n","  IN_COLAB = True\n","\n","except:\n","\n","  IN_COLAB = False\n","\n","\n","\n","if IN_COLAB:\n","\n","    from google.colab import drive\n","\n","\n","\n","    drive.mount('/gdrive')\n","\n","    %cd  /gdrive/My Drive/[2024-2025] AN2DL/Homework 2\n","\n","    !pip install keras_cv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8lGg13S_7tv4","outputId":"b24b8408-c499-4186-f9c5-00308d922d9b","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:06.883680Z","iopub.execute_input":"2024-12-05T11:13:06.884099Z","iopub.status.idle":"2024-12-05T11:13:06.891665Z","shell.execute_reply.started":"2024-12-05T11:13:06.884056Z","shell.execute_reply":"2024-12-05T11:13:06.890708Z"},"executionInfo":{"status":"ok","timestamp":1733778210674,"user_tz":-60,"elapsed":4507,"user":{"displayName":"Leonardo Pompigna","userId":"05449994790436039716"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive/.shortcut-targets-by-id/1rPuqfmPD7NsZJ8nrbnOTfNWKJtu59ajC/Homework 2\n","Requirement already satisfied: keras_cv in /usr/local/lib/python3.10/dist-packages (0.9.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras_cv) (24.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras_cv) (1.4.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from keras_cv) (2024.9.11)\n","Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (from keras_cv) (4.9.7)\n","Requirement already satisfied: keras-core in /usr/local/lib/python3.10/dist-packages (from keras_cv) (0.1.7)\n","Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages (from keras_cv) (0.3.4)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_cv) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kagglehub->keras_cv) (4.66.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_cv) (1.26.4)\n","Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_cv) (13.9.4)\n","Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_cv) (0.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_cv) (3.12.1)\n","Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from keras-core->keras_cv) (0.1.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (8.1.7)\n","Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (4.2.1)\n","Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (2.3)\n","Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (4.25.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (5.9.5)\n","Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (17.0.0)\n","Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (0.1.6)\n","Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (1.13.1)\n","Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (2.5.0)\n","Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (0.10.2)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (1.17.0)\n","Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets->keras_cv) (0.5.1)\n","Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras_cv) (1.11.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras_cv) (4.12.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras_cv) (2024.10.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras_cv) (6.4.5)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->keras_cv) (3.21.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_cv) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_cv) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_cv) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->kagglehub->keras_cv) (2024.8.30)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets->keras_cv) (1.16.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras_cv) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras-core->keras_cv) (2.18.0)\n","Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow-datasets->keras_cv) (0.16)\n","Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets->keras_cv) (1.66.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras-core->keras_cv) (0.1.2)\n"]}],"execution_count":4},{"id":"hlZgYZ6i7ute","cell_type":"markdown","source":["## âš™ï¸ Import Libraries"],"metadata":{"id":"hlZgYZ6i7ute"}},{"id":"6IpcjgO07x6U","cell_type":"code","source":["# Set seed for reproducibility\n","\n","\n","\n","seed = 42\n","\n","\n","\n","\n","\n","\n","\n","# Import necessary libraries\n","\n","\n","\n","import os\n","\n","from datetime import datetime\n","\n","\n","\n","if 'KAGGLE_URL_BASE' in os.environ:\n","\n","    IN_KAGGLE = True\n","\n","else:\n","\n","    IN_KAGGLE = False\n","\n","\n","\n","\n","\n","# Set environment variables before importing modules\n","\n","\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","\n","\n","\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","\n","\n","\n","os.environ['MPLCONFIGDIR'] = os.getcwd() + '/configs/'\n","\n","\n","\n","\n","\n","\n","\n","# Suppress warnings\n","\n","\n","\n","import warnings\n","\n","\n","\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","\n","\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","\n","\n","\n","\n","\n","\n","# Import necessary modules\n","\n","\n","\n","import logging\n","\n","\n","\n","import random\n","\n","\n","\n","import numpy as np\n","\n","\n","\n","\n","\n","\n","\n","# Set seeds for random number generators in NumPy and Python\n","\n","\n","\n","np.random.seed(seed)\n","\n","\n","\n","random.seed(seed)\n","\n","\n","\n","\n","\n","\n","\n","# Import TensorFlow and Keras\n","\n","\n","\n","import tensorflow as tf\n","\n","\n","\n","from tensorflow import keras as tfk\n","\n","\n","\n","from tensorflow.keras import layers as tfkl\n","\n","\n","\n","\n","\n","\n","\n","# Set seed for TensorFlow\n","\n","\n","\n","tf.random.set_seed(seed)\n","\n","\n","\n","tf.compat.v1.set_random_seed(seed)\n","\n","\n","\n","\n","\n","\n","\n","# Reduce TensorFlow verbosity\n","\n","\n","\n","tf.autograph.set_verbosity(0)\n","\n","\n","\n","tf.get_logger().setLevel(logging.ERROR)\n","\n","\n","\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","\n","\n","\n","\n","\n","\n","\n","# Print TensorFlow version\n","\n","\n","\n","print(tf.__version__)\n","\n","\n","\n","\n","\n","\n","\n","# Import other libraries\n","\n","\n","\n","import os\n","\n","\n","\n","import math\n","\n","\n","\n","from PIL import Image\n","\n","\n","\n","from keras import backend as K\n","\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","\n","\n","\n","import matplotlib.pyplot as plt\n","\n","\n","\n","import seaborn as sns\n","\n","\n","\n","import pandas as pd\n","\n","\n","\n","\n","\n","# Configure plot display settings\n","\n","\n","\n","sns.set(font_scale=1.4)\n","\n","\n","\n","sns.set_style('white')\n","\n","\n","\n","plt.rc('font', size=14)\n","\n","\n","\n","%matplotlib inline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6IpcjgO07x6U","outputId":"4fdc8742-53c9-4514-caee-4606c6d5e221","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:06.893369Z","iopub.execute_input":"2024-12-05T11:13:06.893715Z","iopub.status.idle":"2024-12-05T11:13:18.644734Z","shell.execute_reply.started":"2024-12-05T11:13:06.893677Z","shell.execute_reply":"2024-12-05T11:13:18.644189Z"},"executionInfo":{"status":"ok","timestamp":1733778012495,"user_tz":-60,"elapsed":12708,"user":{"displayName":"Leonardo Pompigna","userId":"05449994790436039716"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2.17.1\n"]}],"execution_count":2},{"id":"XkmAUQjyLRM4","cell_type":"markdown","source":["## â³ Load and Process Data"],"metadata":{"id":"XkmAUQjyLRM4"}},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gcm72u2DXfCj","executionInfo":{"status":"ok","timestamp":1733598403710,"user_tz":-60,"elapsed":9,"user":{"displayName":"Olivia Gallego Toscano","userId":"03134782601783665783"}},"outputId":"3bd69cdd-2dd3-4c82-c310-b3021dca198f"},"id":"Gcm72u2DXfCj","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Combining_Augmentations_Training_after_split.ipynb\n","'Copia de Python Crash Course.ipynb'\n","'Copia de TensorFlow2 Crash Course.ipynb'\n"," \u001b[0m\u001b[01;34mdata\u001b[0m/\n"," SMOTE_Segmentation_training_after_split.ipynb\n","'Targeted Cropping_ PreprocessingWithSplit.ipynb'\n"]}]},{"id":"rL8Qr7_5LLq9","cell_type":"code","source":["if IN_KAGGLE:\n","\n","    DATASET_PATH = \"/kaggle/input/and2l-hw2-mars-ground\"\n","\n","elif IN_COLAB:\n","\n","    DATASET_PATH = \"data\"\n","\n","else:\n","\n","    DATASET_PATH = \"data/preprocessed\"\n","\n","\n","\n","\n","\n","#training_set = np.load(DATASET_PATH + \"/training_set_oversampling.npy\")\n","\n","#aÃ±ado el training_set\n","training_set = np.load(DATASET_PATH + \"/training_set.npy\")\n","test_set = np.load(DATASET_PATH + \"/test_set.npy\")\n","\n","\n","#train_img, val_img, train_lbl, val_lbl\n","\n","\n","X_test = test_set\n","\n","\n","train = np.load(DATASET_PATH + \"/preprocessed/training_set_aftersplit_oversampling_crop4.npy\")\n","val = np.load(DATASET_PATH + \"/preprocessed/validation_set_aftersplit_crop.npy\")\n","\n","train_img = train[:,0]\n","train_lbl = train[:,1]\n","val_img = val[:,0]\n","val_lbl = val[:,1]\n","\n","print(len(val))\n","\n","\n","\n","# Add channel dimension\n","\n","if len(train_img.shape) == 3:\n","\n","    train_img = train_img[..., np.newaxis]\n","    val_img = val_img[..., np.newaxis]\n","\n","\n","\n","    train_lbl = train_lbl[..., np.newaxis]\n","    val_lbl = val_lbl[..., np.newaxis]\n","\n","    X_test = X_test[..., np.newaxis]\n","\n","\n","\n","\n","\n","\n","\n","IMAGE_SIZE = train_img.shape[1:]\n","\n","\n","\n","print(f\"Image size: {IMAGE_SIZE}\")\n","\n","\n","\n","print(f\"Training X shape: {train_img.shape}\")\n","\n","\n","\n","print(f\"Training y shape: {train_lbl.shape}\")\n","\n","\n","\n","print(f\"Test X shape: {X_test.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rL8Qr7_5LLq9","outputId":"5081f0f4-7d38-46a0-baeb-c44d509788f4","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:18.645575Z","iopub.execute_input":"2024-12-05T11:13:18.645937Z","iopub.status.idle":"2024-12-05T11:13:21.408095Z","shell.execute_reply.started":"2024-12-05T11:13:18.645910Z","shell.execute_reply":"2024-12-05T11:13:21.407225Z"},"executionInfo":{"status":"ok","timestamp":1733778239735,"user_tz":-60,"elapsed":16621,"user":{"displayName":"Leonardo Pompigna","userId":"05449994790436039716"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["200\n","Image size: (64, 128, 1)\n","Training X shape: (2765, 64, 128, 1)\n","Training y shape: (2765, 64, 128, 1)\n","Test X shape: (10022, 64, 128, 1)\n"]}],"execution_count":6},{"id":"b7efdc03","cell_type":"code","source":["train_img = train_img / 255.0\n","\n","val_img = val_img/255.0\n","\n","X_test = X_test / 255.0"],"metadata":{"id":"b7efdc03","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:22.634713Z","iopub.execute_input":"2024-12-05T11:13:22.635058Z","iopub.status.idle":"2024-12-05T11:13:22.899410Z","shell.execute_reply.started":"2024-12-05T11:13:22.635021Z","shell.execute_reply":"2024-12-05T11:13:22.898457Z"},"executionInfo":{"status":"ok","timestamp":1733778243048,"user_tz":-60,"elapsed":617,"user":{"displayName":"Leonardo Pompigna","userId":"05449994790436039716"}}},"outputs":[],"execution_count":7},{"id":"bGTLiT7yygcR","cell_type":"code","source":["# Set batch size for training\n","\n","\n","\n","BATCH_SIZE = 64\n","\n","\n","\n","\n","\n","\n","\n","# Set learning rate for the optimiser\n","\n","\n","\n","LEARNING_RATE = 1e-3\n","\n","\n","\n","\n","\n","\n","\n","# Set early stopping patience threshold\n","PATIENCE = 20\n","\n","# Set maximum number of training epochs\n","EPOCHS = 1000\n","\n","\n","# Set data split size for training and validation\n","SPLITS_SIZE = 200"],"metadata":{"id":"bGTLiT7yygcR","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:25.908017Z","iopub.execute_input":"2024-12-05T11:13:25.908975Z","iopub.status.idle":"2024-12-05T11:13:25.914045Z","shell.execute_reply.started":"2024-12-05T11:13:25.908931Z","shell.execute_reply":"2024-12-05T11:13:25.913241Z"},"executionInfo":{"status":"ok","timestamp":1733778249305,"user_tz":-60,"elapsed":323,"user":{"displayName":"Leonardo Pompigna","userId":"05449994790436039716"}}},"outputs":[],"execution_count":8},{"id":"6w9C5huBygXN","cell_type":"code","source":["\n","\n","print(f\"\\nNumber of images:\")\n","\n","\n","\n","print(f\"Train: {len(train_img)}\")\n","\n","\n","\n","print(f\"Validation: {len(val_img)}\")\n","\n","\n","\n","print(f\"Test: {len(X_test)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6w9C5huBygXN","outputId":"4513d005-6a95-4c8b-bb38-2a56108c645b","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:25.915319Z","iopub.execute_input":"2024-12-05T11:13:25.916113Z","iopub.status.idle":"2024-12-05T11:13:26.046979Z","shell.execute_reply.started":"2024-12-05T11:13:25.916073Z","shell.execute_reply":"2024-12-05T11:13:26.046047Z"},"executionInfo":{"status":"ok","timestamp":1733778254300,"user_tz":-60,"elapsed":308,"user":{"displayName":"Leonardo Pompigna","userId":"05449994790436039716"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Number of images:\n","Train: 2765\n","Validation: 200\n","Test: 10022\n"]}],"execution_count":9},{"id":"TP4tyMsYNwoN","cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from imblearn.over_sampling import SMOTE\n","\n","def analyze_class_imbalance(y_train):\n","    \"\"\"\n","    Analyze class distribution in the dataset.\n","    Args:\n","        y_train (numpy array): Label dataset (e.g., (2615, 64, 128, 1)).\n","    Returns:\n","        class_counts (dict): Dictionary with class frequencies.\n","    \"\"\"\n","    y_flat = y_train.flatten()\n","    unique_classes, pixel_counts = np.unique(y_flat, return_counts=True)\n","    total_pixels = np.sum(pixel_counts)\n","    class_distribution = pixel_counts / total_pixels\n","\n","    print(\"Class Distribution:\")\n","    for cls, count, proportion in zip(unique_classes, pixel_counts, class_distribution):\n","        print(f\"Class {cls}: {count} pixels ({proportion:.2%})\")\n","\n","    # Plot class distribution\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(unique_classes, pixel_counts, color='skyblue')\n","    plt.xlabel(\"Classes\")\n","    plt.ylabel(\"Pixel Counts\")\n","    plt.title(\"Class Distribution\")\n","    plt.xticks(unique_classes)\n","    plt.show()\n","\n","    return dict(zip(unique_classes, pixel_counts))\n","\n","def calculate_centroid(mask):\n","    \"\"\"\n","    Calculate the centroid of a binary mask.\n","    \"\"\"\n","    indices = np.argwhere(mask)\n","    if indices.size == 0:\n","        return None\n","    centroid = np.mean(indices, axis=0)\n","    return int(centroid[0]), int(centroid[1])\n","\n","def crop_and_resize(image, label, centroid, crop_fraction=0.5):\n","    \"\"\"\n","    Crop around the centroid and resize to original size.\n","    \"\"\"\n","    if centroid is None:\n","        return image, label\n","    h, w = image.shape[:2]\n","    crop_h, crop_w = int(crop_fraction * h), int(crop_fraction * w)\n","    x_c, y_c = centroid\n","    x_min, x_max = max(0, x_c - crop_h // 2), min(h, x_c + crop_h // 2)\n","    y_min, y_max = max(0, y_c - crop_w // 2), min(w, y_c + crop_w // 2)\n","    cropped_image = image[x_min:x_max, y_min:y_max]\n","    cropped_label = label[x_min:x_max, y_min:y_max]\n","    resized_image = tf.image.resize(cropped_image, (h, w), method=\"bilinear\")\n","    resized_label = tf.image.resize(cropped_label, (h, w), method=\"nearest\")\n","    return resized_image.numpy(), resized_label.numpy()\n","\n","def augment_with_targeted_cropping(images, labels, crop_fraction=0.5, num_replicas=5):\n","    \"\"\"\n","    Perform targeted cropping and augmentation for under-represented classes.\n","    \"\"\"\n","    augmented_images, augmented_labels = [], []\n","    for img, lbl in zip(images, labels):\n","        class_4_mask = (lbl == 4)\n","        centroid = calculate_centroid(class_4_mask)\n","        for _ in range(num_replicas):\n","            cropped_img, cropped_lbl = crop_and_resize(img, lbl, centroid, crop_fraction)\n","            augmented_images.append(cropped_img)\n","            augmented_labels.append(cropped_lbl)\n","    return np.array(augmented_images), np.array(augmented_labels)\n","\n","def extract_patches(images, labels, patch_size=16):\n","    \"\"\"\n","    Extract patches containing class 4 for SMOTE.\n","    \"\"\"\n","    patches, patch_labels = [], []\n","    for img, lbl in zip(images, labels):\n","        class_4_mask = (lbl == 4)\n","        if np.sum(class_4_mask) > 0:\n","            # Extract class 4 pixels and ensure correct size\n","            class_pixels = img[class_4_mask]\n","            num_pixels = len(class_pixels)\n","            if num_pixels < patch_size**2:\n","                # Pad if the patch is smaller than the expected size\n","                padding = patch_size**2 - num_pixels\n","                class_pixels = np.pad(class_pixels, (0, padding), mode='constant', constant_values=0)\n","            elif num_pixels > patch_size**2:\n","                # Trim if the patch is larger than the expected size\n","                class_pixels = class_pixels[:patch_size**2]\n","\n","            # Flatten and add to patches\n","            patches.append(class_pixels.flatten())\n","            patch_labels.append(4)  # Label this patch as class 4\n","\n","    return np.array(patches), np.array(patch_labels)\n","\n","\n","def integrate_synthetic_patches(images, labels, synthetic_patches, patch_size=16):\n","    \"\"\"\n","    Integrate synthetic patches back into images.\n","    \"\"\"\n","    h, w, c = images.shape[1:]  # Include channel dimension\n","    synthetic_images, synthetic_labels = [], []\n","    for _ in range(len(synthetic_patches)):\n","        idx = np.random.randint(0, len(images))\n","        img, lbl = images[idx].copy(), labels[idx].copy()\n","\n","        # Reshape synthetic patch to match (patch_size, patch_size, 1)\n","        synthetic_patch = synthetic_patches[_].reshape(patch_size, patch_size, 1)\n","\n","        # Select random position for insertion\n","        x, y = np.random.randint(0, h - patch_size), np.random.randint(0, w - patch_size)\n","\n","        # Insert synthetic patch into image and label\n","        img[x:x + patch_size, y:y + patch_size, :] = synthetic_patch\n","        lbl[x:x + patch_size, y:y + patch_size, :] = 4\n","\n","        synthetic_images.append(img)\n","        synthetic_labels.append(lbl)\n","\n","    return np.array(synthetic_images), np.array(synthetic_labels)\n","\n","# Identify images with class 4\n","minimum_class4 = 0.01\n","mask = np.sum(train_lbl == 4, axis=(1, 2, 3)) > minimum_class4 * train_lbl.shape[1] * train_lbl.shape[2]\n","to_replicate = train_img[mask]\n","to_replicate_labels = train_lbl[mask]\n","\n","# Perform targeted cropping\n","aug_images, aug_labels = augment_with_targeted_cropping(to_replicate, to_replicate_labels)\n","\n","from sklearn.utils import shuffle  # Ensure this import is included\n","\n","# Extract class 4 patches for SMOTE\n","patches, patch_labels = extract_patches(to_replicate, to_replicate_labels)\n","\n","# Ensure patch_labels has at least two classes\n","if np.unique(patch_labels).size == 1:\n","    # Create dummy samples for another class (e.g., class 0)\n","    dummy_patches = np.zeros_like(patches[:10])  # 10 dummy samples\n","    dummy_labels = np.zeros_like(patch_labels[:10])  # Class 0\n","    patches = np.concatenate([patches, dummy_patches], axis=0)\n","    patch_labels = np.concatenate([patch_labels, dummy_labels], axis=0)\n","\n","    # Shuffle the data to mix the dummy samples\n","    patches, patch_labels = shuffle(patches, patch_labels, random_state=42)\n","\n","# Apply SMOTE\n","smote = SMOTE()\n","synthetic_patches, synthetic_labels = smote.fit_resample(patches, patch_labels)\n","\n","# Filter out synthetic patches for the dummy class (retain only class 4)\n","synthetic_patches = synthetic_patches[synthetic_labels == 4]\n","synthetic_labels = synthetic_labels[synthetic_labels == 4]\n","\n","# Integrate synthetic patches into images\n","synthetic_images, synthetic_labels = integrate_synthetic_patches(to_replicate, to_replicate_labels, synthetic_patches)\n","\n","# Combine augmented and synthetic data with the original dataset\n","train_img = np.concatenate([train_img, aug_images, synthetic_images], axis=0)\n","train_lbl = np.concatenate([train_lbl, aug_labels, synthetic_labels], axis=0)\n","\n","print(f\"New Training Dataset Shape: Images: {train_img.shape}, Labels: {train_lbl.shape}\")\n","\n","# Analyze and plot class distribution\n","class_counts = analyze_class_imbalance(train_lbl)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":708},"id":"TP4tyMsYNwoN","outputId":"bb0d4aa0-cd25-43e0-8fdd-902cc6a689b9","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:26.048011Z","iopub.execute_input":"2024-12-05T11:13:26.048303Z","iopub.status.idle":"2024-12-05T11:13:27.602156Z","shell.execute_reply.started":"2024-12-05T11:13:26.048275Z","shell.execute_reply":"2024-12-05T11:13:27.601332Z"},"executionInfo":{"status":"ok","timestamp":1733778293488,"user_tz":-60,"elapsed":9856,"user":{"displayName":"Leonardo Pompigna","userId":"05449994790436039716"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["New Training Dataset Shape: Images: (5681, 64, 128, 1), Labels: (5681, 64, 128, 1)\n","Class Distribution:\n","Class 0.0: 15771971 pixels (33.89%)\n","Class 1.0: 7032777 pixels (15.11%)\n","Class 2.0: 4991126 pixels (10.72%)\n","Class 3.0: 5200367 pixels (11.17%)\n","Class 4.0: 13542511 pixels (29.10%)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":["# save .npy files\n","# cambio X_train_filtered_with_oversampling -> train_img_aftersplit_oversampling4\n","# cambio y_train_filtered_with_oversampling -> train_lbl_aftersplit_oversampling4\n","# cambio training_set_filtered_with_oversampling -> training_set_aftersplit_oversampling4\n","\n","# salvo anche val_img e val_lbl in validation_set_aftersplit\n","\n","train_img_aftersplit_oversampling4 = np.squeeze(train_img, axis=-1)\n","train_lbl_aftersplit_oversampling4 = np.squeeze(train_lbl, axis=-1)\n","val_img = np.squeeze(val_img, axis=-1)\n","val_lbl = np.squeeze(val_lbl, axis=-1)\n","\n","print(train_img_aftersplit_oversampling4.shape)\n","print(train_lbl_aftersplit_oversampling4.shape)\n","print(val_img.shape)\n","print(val_lbl.shape)\n","\n","training_set_aftersplit_oversampling4 = np.zeros((train_img_aftersplit_oversampling4.shape[0], 2, train_img_aftersplit_oversampling4.shape[1], train_img_aftersplit_oversampling4.shape[2]))\n","training_set_aftersplit_oversampling4[:, 0] = train_img_aftersplit_oversampling4\n","training_set_aftersplit_oversampling4[:, 1] = train_lbl_aftersplit_oversampling4\n","validation_set_aftersplit = np.zeros((val_img.shape[0], 2, val_img.shape[1], val_img.shape[2]))\n","validation_set_aftersplit[:,0] = val_img\n","validation_set_aftersplit[:,1] = val_lbl\n","\n","print(training_set_aftersplit_oversampling4.shape)\n","print(validation_set_aftersplit.shape)\n","np.save(\"data/preprocessed/ts_augment4.npy\", training_set_aftersplit_oversampling4)\n","np.save(\"data/preprocessed/vs_augment4.npy\", validation_set_aftersplit)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VrewKZJIR4M3","executionInfo":{"status":"ok","timestamp":1733779064399,"user_tz":-60,"elapsed":3984,"user":{"displayName":"Leonardo Pompigna","userId":"05449994790436039716"}},"outputId":"bf3fffc0-0183-4560-b611-02c0208690a4"},"id":"VrewKZJIR4M3","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["(5681, 64, 128)\n","(5681, 64, 128)\n","(200, 64, 128)\n","(200, 64, 128)\n","(5681, 2, 64, 128)\n","(200, 2, 64, 128)\n"]}]},{"id":"qR2gJuLUNbZc","cell_type":"code","source":["import numpy as np\n","\n","\n","\n","\n","\n","\n","\n","def calculate_class_weights(y_train):\n","\n","\n","\n","    \"\"\"\n","\n","\n","\n","    Calcola i pesi delle classi basandosi sulla distribuzione dei pixel in y_train.\n","\n","\n","\n","\n","\n","\n","\n","    Args:\n","\n","\n","\n","        y_train (numpy array): Dataset di label (es. (2615, 64, 128, 1)).\n","\n","\n","\n","\n","\n","\n","\n","    Returns:\n","\n","\n","\n","        class_weights (dict): Dizionario con i pesi delle classi {classe: peso}.\n","\n","\n","\n","    \"\"\"\n","\n","\n","\n","    # Appiattisci le label per calcolare la frequenza\n","\n","\n","\n","    y_flat = y_train.flatten()\n","\n","\n","\n","\n","\n","\n","\n","    # Trova classi uniche e conta i pixel\n","\n","\n","\n","    unique_classes, pixel_counts = np.unique(y_flat, return_counts=True)\n","\n","\n","\n","\n","\n","\n","\n","    # Calcola i pesi come inverso della frequenza\n","\n","\n","\n","    total_pixels = np.sum(pixel_counts)\n","\n","\n","\n","    class_weights = {cls: total_pixels / (len(unique_classes) * count)\n","\n","\n","\n","                     for cls, count in zip(unique_classes, pixel_counts)}\n","\n","\n","\n","\n","\n","\n","\n","    print(\"Pesi calcolati per le classi:\")\n","\n","\n","\n","    for cls, weight in class_weights.items():\n","\n","\n","\n","        print(f\"Classe {cls}: Peso {weight:.4f}\")\n","\n","\n","\n","\n","\n","\n","\n","    return class_weights\n","\n","\n","\n","\n","\n","\n","\n","# Calcola i pesi delle classi\n","\n","\n","\n","class_weights = calculate_class_weights(train_lbl)\n"],"metadata":{"id":"qR2gJuLUNbZc","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:27.605349Z","iopub.execute_input":"2024-12-05T11:13:27.605664Z","iopub.status.idle":"2024-12-05T11:13:28.873924Z","shell.execute_reply.started":"2024-12-05T11:13:27.605635Z","shell.execute_reply":"2024-12-05T11:13:28.873016Z"}},"outputs":[],"execution_count":null},{"id":"3bqBGLa7JajV","cell_type":"code","source":["class_weights[0] = 0#results improved by loss that not consider the background"],"metadata":{"id":"3bqBGLa7JajV","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:28.874804Z","iopub.execute_input":"2024-12-05T11:13:28.875044Z","iopub.status.idle":"2024-12-05T11:13:28.879407Z","shell.execute_reply.started":"2024-12-05T11:13:28.875019Z","shell.execute_reply":"2024-12-05T11:13:28.878332Z"}},"outputs":[],"execution_count":null},{"id":"WVnWhprMHJ9q","cell_type":"code","source":["# Definizione della category mapping\n","\n","\n","\n","category_map = {\n","\n","\n","\n","    0: 0,  # Background -> Background\n","\n","\n","\n","    1: 1,  # Soil -> Soil\n","\n","\n","\n","    2: 2,  # Bedrock -> Bedrock\n","\n","\n","\n","    3: 3,  # Sand -> Sand\n","\n","\n","\n","    4: 4,  # Big Rock -> Big Rock\n","\n","\n","\n","}\n","\n","\n","\n","NUM_CLASSES = 5  # Da 0 a 4\n","\n","\n","\n","print(f\"Number of classes: {NUM_CLASSES}\")"],"metadata":{"id":"WVnWhprMHJ9q","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:28.880485Z","iopub.execute_input":"2024-12-05T11:13:28.880743Z","iopub.status.idle":"2024-12-05T11:13:28.887980Z","shell.execute_reply.started":"2024-12-05T11:13:28.880715Z","shell.execute_reply":"2024-12-05T11:13:28.887213Z"}},"outputs":[],"execution_count":null},{"id":"wvqFgT7pygSf","cell_type":"code","source":["def load_single_image(image, label, input_size=(64, 128)):\n","\n","\n","\n","    \"\"\"\n","\n","\n","\n","    Preprocessa un'immagine e la sua label.\n","\n","\n","\n","    \"\"\"\n","\n","\n","\n","    # Resize e normalizza l'immagine\n","\n","\n","\n","    image = tf.image.resize(image, input_size)\n","\n","\n","\n","    image = tf.cast(image, tf.float32)\n","\n","\n","\n","\n","\n","\n","\n","    # Resize la label\n","\n","\n","\n","    label = tf.image.resize(label, input_size, method='nearest')  # Metodo adatto per label discrete\n","\n","\n","\n","    label = tf.cast(label, tf.int32)  # Assicurati che le label siano interi\n","\n","\n","\n","\n","\n","\n","\n","    return image, label\n"],"metadata":{"id":"wvqFgT7pygSf","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:28.888851Z","iopub.execute_input":"2024-12-05T11:13:28.889058Z","iopub.status.idle":"2024-12-05T11:13:28.896074Z","shell.execute_reply.started":"2024-12-05T11:13:28.889037Z","shell.execute_reply":"2024-12-05T11:13:28.895220Z"}},"outputs":[],"execution_count":null},{"id":"m7CfNte5mleo","cell_type":"code","source":["def apply_category_mapping(label):\n","\n","\n","\n","    \"\"\"\n","\n","\n","\n","    Apply category mapping to labels.\n","\n","\n","\n","    \"\"\"\n","\n","\n","\n","    keys_tensor = tf.constant(list(category_map.keys()), dtype=tf.int32)\n","\n","\n","\n","    vals_tensor = tf.constant(list(category_map.values()), dtype=tf.int32)\n","\n","\n","\n","    table = tf.lookup.StaticHashTable(\n","\n","\n","\n","        tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),\n","\n","\n","\n","        default_value=0\n","\n","\n","\n","    )\n","\n","\n","\n","    return table.lookup(label)"],"metadata":{"id":"m7CfNte5mleo","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:28.897152Z","iopub.execute_input":"2024-12-05T11:13:28.897481Z","iopub.status.idle":"2024-12-05T11:13:28.907233Z","shell.execute_reply.started":"2024-12-05T11:13:28.897456Z","shell.execute_reply":"2024-12-05T11:13:28.906412Z"}},"outputs":[],"execution_count":null},{"id":"qvbEqWpDygP3","cell_type":"code","source":["\n","\n","@tf.function\n","\n","\n","\n","def random_flip(image, label, seed=None):\n","\n","\n","\n","    #Consistent random horizontal flip.\n","\n","\n","\n","    if seed is None:\n","\n","\n","\n","        seed = np.random.randint(0, 1000000)\n","\n","\n","\n","    flip_prob = tf.random.uniform([], seed=seed)\n","\n","\n","\n","    image = tf.cond(\n","\n","\n","\n","        flip_prob > 0.5,\n","\n","\n","\n","        lambda: tf.image.flip_left_right(image),\n","\n","\n","\n","        lambda: image\n","\n","\n","\n","    )\n","\n","\n","\n","    label = tf.cond(\n","\n","\n","\n","        flip_prob > 0.5,\n","\n","\n","\n","        lambda: tf.image.flip_left_right(label),\n","\n","\n","\n","        lambda: label\n","\n","\n","\n","    )\n","\n","\n","\n","    return image, label\n"],"metadata":{"id":"qvbEqWpDygP3","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:28.908280Z","iopub.execute_input":"2024-12-05T11:13:28.908910Z","iopub.status.idle":"2024-12-05T11:13:28.921660Z","shell.execute_reply.started":"2024-12-05T11:13:28.908872Z","shell.execute_reply":"2024-12-05T11:13:28.920954Z"}},"outputs":[],"execution_count":null},{"id":"wxCdz4ixThHg","cell_type":"code","source":["@tf.function\n","\n","def grid_mask(image, label, seed=None, d_min=10, d_max=50, r_min=0.3, r_max=0.7):\n","\n","    \"\"\"\n","\n","    Applica Grid Mask su un'immagine.\n","\n","    Parametri:\n","\n","    - d_min, d_max: Range della dimensione del quadrato della griglia.\n","\n","    - r_min, r_max: Proporzione di mascheratura nella griglia.\n","\n","    \"\"\"\n","\n","    if seed is None:\n","\n","        seed = tf.random.uniform([], maxval=1000000, dtype=tf.int32)\n","\n","\n","\n","    # Estrai dimensioni immagine\n","\n","    height, width, channels = tf.unstack(tf.shape(image))\n","\n","\n","\n","    # Dimensione casuale della cella della griglia\n","\n","    d = tf.random.uniform([], minval=d_min, maxval=d_max, dtype=tf.int32, seed=seed)\n","\n","\n","\n","    # Proporzione di mascheratura casuale\n","\n","    r = tf.random.uniform([], minval=r_min, maxval=r_max, dtype=tf.float32, seed=seed)\n","\n","\n","\n","    # Creazione della maschera di base\n","\n","    cut = tf.cast(tf.round(r * tf.cast(d, tf.float32)), tf.int32)\n","\n","    mask_row = tf.concat([tf.ones((cut, d)), tf.zeros((d - cut, d))], axis=0)\n","\n","    mask = tf.tile(mask_row, (height // d + 1, width // d + 1))\n","\n","    mask = mask[:height, :width]  # Ritaglio alle dimensioni esatte\n","\n","\n","\n","    # Inverti la maschera per applicare l'effetto di Grid Mask\n","\n","    mask = 1 - mask\n","\n","\n","\n","    # Aggiungi una dimensione per il canale\n","\n","    mask = tf.expand_dims(mask, axis=-1)\n","\n","\n","\n","    # Ridimensiona la maschera se necessario\n","\n","    mask = tf.image.resize_with_crop_or_pad(mask, height, width)\n","\n","\n","\n","    # Applica Grid Mask all'immagine\n","\n","    image = image * mask\n","\n","\n","\n","    return image, label\n"],"metadata":{"id":"wxCdz4ixThHg","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:28.922608Z","iopub.execute_input":"2024-12-05T11:13:28.922856Z","iopub.status.idle":"2024-12-05T11:13:28.931357Z","shell.execute_reply.started":"2024-12-05T11:13:28.922833Z","shell.execute_reply":"2024-12-05T11:13:28.930648Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["ALBUMENTATIONS FOR AUGMENTATIONS\n"],"metadata":{"id":"TKdJlVUATf1i"},"id":"TKdJlVUATf1i"},{"cell_type":"code","source":["!pip install -U albumentations"],"metadata":{"id":"gZBJFfb4TjDe"},"id":"gZBJFfb4TjDe","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Definisci le trasformazioni di Albumentations\n","def get_augmentations():\n","    \"\"\"\n","    Definisce trasformazioni aggressive di Albumentations.\n","    \"\"\"\n","    return A.Compose([\n","        A.HorizontalFlip(p=0.7),              # Aumenta la probabilitÃ  di flip orizzontale\n","        A.VerticalFlip(p=0.7),                # Aumenta la probabilitÃ  di flip verticale\n","        A.Rotate(limit=45, p=0.9),            # Rotazione casuale entro Â±45 gradi\n","        A.ElasticTransform(alpha=150, sigma=20, alpha_affine=30, p=0.5),  # Deformazioni elastiche piÃ¹ forti\n","        A.GridDistortion(num_steps=5, distort_limit=0.4, p=0.5),  # Distorsione della griglia piÃ¹ intensa\n","    ])\n","\n","# 2. Funzione per applicare Albumentations\n","def albumentations_augment(image, mask):\n","    \"\"\"\n","    Applica trasformazioni aggressive con Albumentations.\n","    - image: array numpy normalizzato (float32, range [0, 1]).\n","    - mask: array numpy con classi (interi).\n","    \"\"\"\n","    # Converti il tensor TensorFlow in array NumPy\n","    image = image.numpy()\n","    mask = mask.numpy()\n","\n","    # Applica le trasformazioni\n","    aug = get_augmentations()\n","    augmented = aug(image=image, mask=mask)\n","    image_aug, mask_aug = augmented['image'], augmented['mask']\n","\n","    # Assicurati che i dati ritornino come float32\n","    return image_aug.astype(np.float32), mask_aug.astype(np.float32)\n","\n","# 3. Wrapper per TensorFlow\n","def augment_with_albumentations(image, mask):\n","    \"\"\"\n","    Wrapper per integrare Albumentations con tf.data.Dataset.\n","    \"\"\"\n","    image, mask = tf.py_function(\n","        func=albumentations_augment,\n","        inp=[image, mask],\n","        Tout=[tf.float32, tf.float32]\n","    )\n","    # Mantieni la shape statica per TensorFlow\n","    image.set_shape([64, 128, 1])  # Specifica la dimensione dell'immagine\n","    mask.set_shape([64, 128, 1])   # Specifica la dimensione della maschera\n","    return image, mask\n"],"metadata":{"id":"a-ae8O-YSWYs"},"id":"a-ae8O-YSWYs","execution_count":null,"outputs":[]},{"id":"cL0Nf8teywO5","cell_type":"code","source":["def make_dataset(images, labels, batch_size, shuffle=True, augment=False, seed=None):\n","\n","\n","\n","    \"\"\"\n","\n","\n","\n","    Create a memory-efficient TensorFlow dataset.\n","\n","\n","\n","    \"\"\"\n","\n","\n","\n","    # Create dataset from file paths\n","\n","\n","\n","    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n","\n","\n","    if shuffle:\n","\n","\n","\n","        dataset = dataset.shuffle(buffer_size=batch_size * 2, seed=seed)\n","\n","\n","    # Load images and labels\n","\n","    dataset = dataset.map(\n","\n","\n","\n","        load_single_image,\n","\n","\n","\n","        num_parallel_calls=tf.data.AUTOTUNE\n","\n","\n","\n","    )\n","\n","\n","    # Apply category mapping\n","\n","\n","\n","\n","\n","\n","\n","    if augment:\n","      dataset = dataset.map(\n","\n","\n","\n","            lambda x, y: random_flip(x, y, seed=seed),\n","\n","\n","\n","            num_parallel_calls=tf.data.AUTOTUNE\n","\n","\n","\n","        )\n","\n","\n","      #dataset = dataset.map(\n","          #  augment_with_albumentations,\n","         #   num_parallel_calls=tf.data.AUTOTUNE\n","        #)\n","\n","\n","\n","    # Batch the data\n","\n","\n","\n","    dataset = dataset.batch(batch_size, drop_remainder=False)\n","\n","\n","\n","    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n","\n","    return dataset"],"metadata":{"id":"cL0Nf8teywO5","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:28.932374Z","iopub.execute_input":"2024-12-05T11:13:28.932911Z","iopub.status.idle":"2024-12-05T11:13:28.945260Z","shell.execute_reply.started":"2024-12-05T11:13:28.932873Z","shell.execute_reply":"2024-12-05T11:13:28.944537Z"}},"outputs":[],"execution_count":null},{"id":"aF8y7CjoywL_","cell_type":"code","source":["# Create the datasets\n","\n","\n","\n","print(\"Creating datasets...\")\n","\n","\n","\n","train_dataset = make_dataset(\n","\n","\n","\n","    train_img, train_lbl,\n","\n","\n","\n","    batch_size=BATCH_SIZE,\n","\n","\n","\n","    shuffle=True,\n","\n","\n","\n","    augment=True,\n","\n","\n","\n","    seed=seed\n","\n","\n","\n",")\n","\n","\n","\n","\n","\n","\n","\n","val_dataset = make_dataset(\n","\n","\n","\n","    val_img, val_lbl,\n","\n","\n","\n","    batch_size=BATCH_SIZE,\n","\n","\n","\n","    shuffle=False\n","\n","\n","\n",")\n","\n","\n","\n","\"\"\"\n","\n","\n","\n","test_dataset = make_dataset(\n","\n","\n","\n","    test_img, test_lbl,\n","\n","\n","\n","    batch_size=BATCH_SIZE,\n","\n","\n","\n","    shuffle=False\n","\n","\n","\n",")\n","\n","\n","\n","\"\"\"\n","\n","\n","\n","print(\"Datasets created!\")\n","\n","\n","\n","\n","\n","\n","\n","# Check the shape of the data\n","\n","\n","\n","for images, labels in train_dataset.take(1):\n","\n","\n","\n","    input_shape = images.shape[1:]\n","\n","\n","\n","    print(f\"\\nInput shape: {input_shape}\")\n","\n","\n","\n","    print(\"Images shape:\", images.shape)\n","\n","\n","\n","    print(\"Labels shape:\", labels.shape)\n","\n","\n","\n","    print(\"Labels dtype:\", labels.dtype)\n","\n","\n","\n","    break"],"metadata":{"id":"aF8y7CjoywL_","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:28.946199Z","iopub.execute_input":"2024-12-05T11:13:28.946462Z","iopub.status.idle":"2024-12-05T11:13:32.783351Z","shell.execute_reply.started":"2024-12-05T11:13:28.946438Z","shell.execute_reply":"2024-12-05T11:13:32.782496Z"}},"outputs":[],"execution_count":null},{"id":"Zd3C28U1ywJM","cell_type":"code","source":["\n","\n","def create_segmentation_colormap(num_classes):\n","\n","    \"\"\"\n","\n","    Create a linear colormap using a predefined palette.\n","\n","    Uses 'viridis' as default because it is perceptually uniform\n","\n","    and works well for colorblindness.\n","\n","    \"\"\"\n","\n","    return plt.cm.viridis(np.linspace(0, 1, num_classes))\n","\n","\n","\n","COLORMAP = create_segmentation_colormap(NUM_CLASSES)\n","\n","\n","\n","def apply_colormap(label, colormap=None):\n","\n","    \"\"\"\n","\n","    Apply the colormap to a label.\n","\n","    \"\"\"\n","\n","    # Ensure label is 2D\n","\n","    label = np.squeeze(label)\n","\n","\n","\n","    colormap = COLORMAP\n","\n","\n","\n","    # Validate colormap size\n","\n","    max_label = label.max()\n","\n","    if max_label >= colormap.shape[0]:\n","\n","        raise ValueError(f\"Label contains value {max_label}, but colormap has only {colormap.shape[0]} colors.\")\n","\n","\n","\n","\n","\n","    # Apply the colormap\n","\n","    colored = colormap[label.astype(int)]\n","\n","\n","\n","    return colored\n","\n","\n","\n","def plot_sample_batch(dataset, num_samples=3):\n","\n","    \"\"\"\n","\n","    Display some image and label pairs from the dataset.\n","\n","    \"\"\"\n","\n","    plt.figure(figsize=(15, 4*num_samples))\n","\n","\n","\n","    for images, labels in dataset.take(1):\n","\n","        labels_np = labels.numpy()\n","\n","        num_classes = len(np.unique(labels_np))\n","\n","        colormap = COLORMAP\n","\n","\n","\n","        for j in range(min(num_samples, len(images))):\n","\n","            # Plot original image\n","\n","            plt.subplot(num_samples, 2, j*2 + 1)\n","\n","            plt.imshow(images[j])\n","\n","            plt.title(f'Image {j+1}')\n","\n","            plt.axis('off')\n","\n","\n","\n","            # Plot colored label\n","\n","            plt.subplot(num_samples, 2, j*2 + 2)\n","\n","            colored_label = apply_colormap(labels_np[j], colormap)\n","\n","            plt.imshow(colored_label)\n","\n","            plt.title(f'Label {j+1}')\n","\n","            plt.axis('off')\n","\n","\n","\n","    plt.tight_layout()\n","\n","    plt.show()\n","\n","    plt.close()\n","\n","\n","\n","print(train_img.shape)\n","\n","# Visualize examples from the training set\n","\n","print(\"Visualizing examples from the training set:\")\n","\n","plot_sample_batch(train_dataset, num_samples=3)"],"metadata":{"id":"Zd3C28U1ywJM","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:13:32.784792Z","iopub.execute_input":"2024-12-05T11:13:32.785142Z","iopub.status.idle":"2024-12-05T11:13:34.252019Z","shell.execute_reply.started":"2024-12-05T11:13:32.785103Z","shell.execute_reply":"2024-12-05T11:13:34.251125Z"}},"outputs":[],"execution_count":null},{"id":"lGKs0aKsfZrj","cell_type":"markdown","source":["## ðŸ› ï¸ Models and Experiments"],"metadata":{"id":"lGKs0aKsfZrj"}},{"id":"RuEAFQKoywD1","cell_type":"code","source":["\n","\n","import keras_cv\n","\"\"\"\n","def unet_block(input_tensor, filters, kernel_size=3, activation='relu', stack=2, name=''):\n","\n","\n","\n","    # Initialise the input tensor\n","\n","\n","\n","    x = input_tensor\n","\n","\n","\n","\n","\n","\n","\n","    # Apply a sequence of Conv2D, Batch Normalisation, and Activation layers for the specified number of stacks\n","\n","\n","\n","    for i in range(stack):\n","\n","\n","\n","        x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', name=name + 'conv' + str(i + 1))(x)\n","\n","\n","\n","        x = tfkl.BatchNormalization(name=name + 'bn' + str(i + 1))(x)\n","\n","\n","\n","        x = tfkl.Activation(activation, name=name + 'activation' + str(i + 1))(x)\n","\n","\n","\n","\n","\n","\n","\n","    # Return the transformed tensor\n","\n","\n","\n","    return x\n","\"\"\"\n","def unet_block(input_tensor, filters, kernel_size=3, activation='relu', stack=2, name=''):\n","    \"\"\"\n","    Blocco UNet con residual connections.\n","    - input_tensor: Tensor di input.\n","    - filters: Numero di filtri per le convoluzioni.\n","    - kernel_size: Dimensione del kernel convoluzionale.\n","    - activation: Funzione di attivazione.\n","    - stack: Numero di convoluzioni nello stack.\n","    - name: Nome del blocco.\n","    \"\"\"\n","    x = input_tensor\n","    shortcut = input_tensor  # Connessione residua\n","\n","    for i in range(stack):\n","        x = tfkl.Conv2D(filters, kernel_size=kernel_size, padding='same', name=f\"{name}conv{i + 1}\")(x)\n","        x = tfkl.BatchNormalization(name=f\"{name}bn{i + 1}\")(x)\n","        x = tfkl.Activation(activation, name=f\"{name}activation{i + 1}\")(x)\n","\n","    # Aggiungi la connessione residua\n","    if shortcut.shape[-1] != filters:  # Adatta i canali se necessario\n","        shortcut = tfkl.Conv2D(filters, kernel_size=1, padding='same', name=f\"{name}shortcut\")(shortcut)\n","\n","    x = tfkl.Add(name=f\"{name}add\")([x, shortcut])\n","    x = tfkl.Activation(activation, name=f\"{name}residual_activation\")(x)\n","\n","    return x\n","\n","\n","\n","def squeeze_and_excite_block(input_tensor, ratio=16):\n","    \"\"\"\n","    Squeeze-and-Excitation block to recalibrate channel-wise feature responses.\n","\n","    Args:\n","        input_tensor: Input feature map.\n","        ratio: Reduction ratio for the dense layers in the SE block.\n","\n","    Returns:\n","        Output tensor with recalibrated features.\n","    \"\"\"\n","    channel_axis = -1  # Assuming 'channels_last' format\n","    filters = input_tensor.shape[channel_axis]\n","\n","    # Squeeze: Global Average Pooling\n","    squeeze = tfkl.GlobalAveragePooling2D()(input_tensor)\n","\n","    # Excitation: Fully connected layers with reduction ratio\n","    excitation = tfkl.Dense(filters // ratio, activation=\"relu\")(squeeze)\n","    excitation = tfkl.Dense(filters, activation=\"sigmoid\")(excitation)\n","\n","    # Reshape and scale\n","    excitation = tfkl.Reshape((1, 1, filters))(excitation)\n","    output_tensor = tfkl.multiply([input_tensor, excitation])\n","    return output_tensor\n","\n","\n","\n","\n","def get_unet_model(input_shape=(64, 128, 1), num_classes=NUM_CLASSES, seed=seed):\n","\n","    tf.random.set_seed(seed)\n","\n","    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n","\n","\n","\n","    # Aggiungi Grid Mask\n","\n","    input_layer = keras_cv.layers.GridMask(\n","\n","        ratio_factor=(0.2, 0.4),    # Proporzione di maschera oscurata\n","\n","        rotation_factor=0.1,       # Rotazione casuale\n","\n","        fill_mode=\"constant\"       # Riempimento delle aree oscurate\n","\n","    )(input_layer)\n","\n","\n","\n","    # Downsampling path\n","\n","    down_block_1 = unet_block(input_layer, 32, name='down_block1_')\n","\n","    d1 = tfkl.MaxPooling2D()(down_block_1)\n","\n","\n","\n","    down_block_2 = unet_block(d1, 64, name='down_block2_')\n","\n","    d2 = tfkl.MaxPooling2D()(down_block_2)\n","\n","\n","\n","    # Bottleneck\n","\n","    bottleneck = unet_block(d2, 128, name='bottleneck')\n","    # Add squeeze and excitation block to the bottleneck to make it more informative\n","    bottleneck = squeeze_and_excite_block(bottleneck, ratio=16)\n","\n","\n","    # Upsampling path\n","\n","    u1 = tfkl.UpSampling2D()(bottleneck)\n","\n","    u1 = tfkl.Concatenate()([u1, down_block_2])\n","\n","    u1 = unet_block(u1, 64, name='up_block1_')\n","\n","\n","\n","    u2 = tfkl.UpSampling2D()(u1)\n","\n","    u2 = tfkl.Concatenate()([u2, down_block_1])\n","\n","    u2 = unet_block(u2, 32, name='up_block2_')\n","\n","\n","\n","    # Output Layer\n","\n","    output_layer = tfkl.Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\", name='output_layer')(u2)\n","\n","\n","\n","    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='UNet')\n","\n","    return model\n","\n","\n","\n","\n","\n","model = get_unet_model(input_shape=IMAGE_SIZE)\n","\n","\n","\n","\n","\n","\n","\n","# Print a detailed summary of the model with expanded nested layers and trainable parameters.\n","\n","\n","\n","model.summary(expand_nested=True, show_trainable=True)\n","\n","\n","\n","\n","\n","\n","\n","# Generate and display a graphical representation of the model architecture.\n","\n","\n","\n","#tf.keras.utils.plot_model(model, show_trainable=True, expand_nested=True, dpi=70)\n"],"metadata":{"id":"RuEAFQKoywD1","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:45:26.831198Z","iopub.execute_input":"2024-12-05T11:45:26.831519Z","iopub.status.idle":"2024-12-05T11:45:27.453769Z","shell.execute_reply.started":"2024-12-05T11:45:26.831492Z","shell.execute_reply":"2024-12-05T11:45:27.452977Z"}},"outputs":[],"execution_count":null},{"id":"zD1Ohgjo1dSO","cell_type":"code","source":["# Define custom Mean Intersection Over Union metric\n","\n","\n","\n","class MeanIntersectionOverUnion(tf.keras.metrics.MeanIoU):\n","\n","\n","\n","    def __init__(self, num_classes, labels_to_exclude=None, name=\"mean_iou\", dtype=None):\n","\n","\n","\n","        super(MeanIntersectionOverUnion, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n","\n","\n","\n","        if labels_to_exclude is None:\n","\n","\n","\n","            labels_to_exclude = [0]  # Default to excluding label 0\n","\n","\n","\n","        self.labels_to_exclude = labels_to_exclude\n","\n","\n","\n","\n","\n","\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","\n","\n","\n","        # Convert predictions to class labels\n","\n","\n","\n","        y_pred = tf.math.argmax(y_pred, axis=-1)\n","\n","\n","\n","\n","\n","\n","\n","        # Flatten the tensors\n","\n","\n","\n","        y_true = tf.reshape(y_true, [-1])\n","\n","\n","\n","        y_pred = tf.reshape(y_pred, [-1])\n","\n","\n","\n","\n","\n","\n","\n","        # Apply mask to exclude specified labels\n","\n","\n","\n","        for label in self.labels_to_exclude:\n","\n","\n","\n","            mask = tf.not_equal(y_true, label)\n","\n","\n","\n","            y_true = tf.boolean_mask(y_true, mask)\n","\n","\n","\n","            y_pred = tf.boolean_mask(y_pred, mask)\n","\n","\n","\n","\n","\n","\n","\n","        # Update the state\n","\n","\n","\n","        return super().update_state(y_true, y_pred, sample_weight)\n","\n","\n","\n","\n","\n","\n","\n","# Visualization callback\n","\n","\n","\n","class VizCallback(tf.keras.callbacks.Callback):\n","\n","\n","\n","    def __init__(self, image_path, label_path, frequency=5):\n","\n","\n","\n","        super().__init__()\n","\n","\n","\n","        self.image_path = image_path\n","\n","\n","\n","        self.label_path = label_path\n","\n","\n","\n","        self.frequency = frequency\n","\n","\n","\n","\n","\n","\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","\n","\n","\n","        if epoch % self.frequency == 0:  # Visualize only every \"frequency\" epochs\n","\n","\n","\n","            image, label = load_single_image(self.image_path, self.label_path)\n","\n","\n","\n","            label = apply_category_mapping(label)\n","\n","\n","\n","            image = tf.expand_dims(image, 0)\n","\n","\n","\n","            pred = self.model.predict(image, verbose=0)\n","\n","\n","\n","            y_pred = tf.math.argmax(pred, axis=-1)\n","\n","\n","\n","            y_pred = y_pred.numpy()\n","\n","\n","\n","\n","\n","\n","\n","            # Create colormap\n","\n","\n","\n","            num_classes = NUM_CLASSES\n","\n","\n","\n","            colormap = create_segmentation_colormap(num_classes)\n","\n","\n","\n","\n","\n","\n","\n","            plt.figure(figsize=(16, 4))\n","\n","\n","\n","\n","\n","\n","\n","            # Input image\n","\n","\n","\n","            plt.subplot(1, 3, 1)\n","\n","\n","\n","            plt.imshow(image[0])\n","\n","\n","\n","            plt.title(\"Input Image\")\n","\n","\n","\n","            plt.axis('off')\n","\n","\n","\n","\n","\n","\n","\n","            # Ground truth\n","\n","\n","\n","            plt.subplot(1, 3, 2)\n","\n","\n","\n","            colored_label = apply_colormap(label.numpy(), colormap)\n","\n","\n","\n","            plt.imshow(colored_label)\n","\n","\n","\n","            plt.title(\"Ground Truth Mask\")\n","\n","\n","\n","            plt.axis('off')\n","\n","\n","\n","\n","\n","\n","\n","            # Prediction\n","\n","\n","\n","            plt.subplot(1, 3, 3)\n","\n","\n","\n","            colored_pred = apply_colormap(y_pred[0], colormap)\n","\n","\n","\n","            plt.imshow(colored_pred)\n","\n","\n","\n","            plt.title(\"Predicted Mask\")\n","\n","\n","\n","            plt.axis('off')\n","\n","\n","\n","\n","\n","\n","\n","            plt.tight_layout()\n","\n","\n","\n","            plt.show()\n","\n","\n","\n","#            plt.close()"],"metadata":{"id":"zD1Ohgjo1dSO","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:45:59.541786Z","iopub.execute_input":"2024-12-05T11:45:59.542119Z","iopub.status.idle":"2024-12-05T11:45:59.554555Z","shell.execute_reply.started":"2024-12-05T11:45:59.542089Z","shell.execute_reply":"2024-12-05T11:45:59.553612Z"}},"outputs":[],"execution_count":null},{"id":"lUvJrFi2VxB2","cell_type":"code","source":["# Define custom Mean Intersection Over Union metric\n","\n","\n","\n","class IoUPerClass(tf.keras.metrics.Metric):\n","\n","    def __init__(self, num_classes, labels_to_exclude=[0], name='iou_per_class', **kwargs):\n","\n","        super(IoUPerClass, self).__init__(name=name, **kwargs)\n","\n","        self.num_classes = num_classes\n","\n","        self.labels_to_exclude = labels_to_exclude\n","\n","\n","\n","        # Usa tf.Variable per creare variabili mutabili\n","\n","        self.true_positives = tf.Variable(tf.zeros((num_classes,), dtype=tf.float32), trainable=False)\n","\n","        self.false_positives = tf.Variable(tf.zeros((num_classes,), dtype=tf.float32), trainable=False)\n","\n","        self.false_negatives = tf.Variable(tf.zeros((num_classes,), dtype=tf.float32), trainable=False)\n","\n","\n","\n","    def update_state(self, y_true, y_pred, sample_weight=None):\n","\n","\n","\n","        y_pred = tf.math.argmax(y_pred, axis=-1)\n","\n","\n","\n","        # Assicurati che y_true e y_pred siano in formato int32\n","\n","        y_true = tf.cast(y_true, dtype=tf.int32)\n","\n","        y_pred = tf.cast(y_pred, dtype=tf.int32)\n","\n","\n","\n","        # Appiattisci y_true e y_pred per confrontarli elemento per elemento\n","\n","        y_true = tf.reshape(y_true, [-1])\n","\n","        y_pred = tf.reshape(y_pred, [-1])\n","\n","\n","\n","        # Crea una maschera per escludere le etichette specificate\n","\n","        mask = tf.reduce_all(tf.not_equal(tf.expand_dims(y_true, -1), self.labels_to_exclude), axis=-1)\n","\n","\n","\n","        # Applica la maschera per escludere i valori indesiderati\n","\n","        y_true = tf.boolean_mask(y_true, mask)\n","\n","        y_pred = tf.boolean_mask(y_pred, mask)\n","\n","\n","\n","        # Calcola i veri positivi (TP), falsi positivi (FP) e falsi negativi (FN) per ogni classe\n","\n","        true_positives_per_class = []\n","\n","        false_positives_per_class = []\n","\n","        false_negatives_per_class = []\n","\n","\n","\n","        for class_id in range(self.num_classes):\n","\n","            true_positive = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, class_id), tf.equal(y_pred, class_id)), tf.float32))\n","\n","            false_positive = tf.reduce_sum(tf.cast(tf.logical_and(tf.not_equal(y_true, class_id), tf.equal(y_pred, class_id)), tf.float32))\n","\n","            false_negative = tf.reduce_sum(tf.cast(tf.logical_and(tf.equal(y_true, class_id), tf.not_equal(y_pred, class_id)), tf.float32))\n","\n","\n","\n","            true_positives_per_class.append(true_positive)\n","\n","            false_positives_per_class.append(false_positive)\n","\n","            false_negatives_per_class.append(false_negative)\n","\n","\n","\n","        # Converte le liste in tensori\n","\n","        true_positives_per_class = tf.convert_to_tensor(true_positives_per_class, dtype=tf.float32)\n","\n","        false_positives_per_class = tf.convert_to_tensor(false_positives_per_class, dtype=tf.float32)\n","\n","        false_negatives_per_class = tf.convert_to_tensor(false_negatives_per_class, dtype=tf.float32)\n","\n","\n","\n","        # Aggiorna i contatori per TP, FP e FN\n","\n","        self.true_positives.assign_add(true_positives_per_class)\n","\n","        self.false_positives.assign_add(false_positives_per_class)\n","\n","        self.false_negatives.assign_add(false_negatives_per_class)\n","\n","\n","\n","    def result(self):\n","\n","        # Calcola l'IoU per ciascuna classe\n","\n","        iou_per_class = tf.where(\n","\n","            tf.math.greater(self.true_positives + self.false_positives + self.false_negatives, 0),\n","\n","            self.true_positives / (self.true_positives + self.false_positives + self.false_negatives),\n","\n","            tf.zeros_like(self.true_positives),\n","\n","        )\n","\n","        return iou_per_class\n","\n","\n","\n","    def reset_state(self):\n","\n","        # Reset dei contatori per TP, FP, FN\n","\n","        self.true_positives.assign(tf.zeros_like(self.true_positives))\n","\n","        self.false_positives.assign(tf.zeros_like(self.false_positives))\n","\n","        self.false_negatives.assign(tf.zeros_like(self.false_negatives))\n"],"metadata":{"id":"lUvJrFi2VxB2","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:45:59.556634Z","iopub.execute_input":"2024-12-05T11:45:59.557138Z","iopub.status.idle":"2024-12-05T11:45:59.571480Z","shell.execute_reply.started":"2024-12-05T11:45:59.557102Z","shell.execute_reply":"2024-12-05T11:45:59.570647Z"}},"outputs":[],"execution_count":null},{"id":"M0a1ax5GHp4W","cell_type":"code","source":["def dice_loss(y_true, y_pred):\n","\n","    smooth = 1e-6\n","\n","\n","\n","    # Numero di classi dal modello (y_pred)\n","\n","    num_classes = tf.shape(y_pred)[-1]\n","\n","\n","\n","    # Converti y_true in one-hot\n","\n","    y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes)\n","\n","\n","\n","    # Rimuovi la dimensione extra (se presente)\n","\n","    y_true_one_hot = tf.squeeze(y_true_one_hot, axis=-2)\n","\n","\n","\n","    # Applica softmax a y_pred\n","\n","    y_pred = tf.nn.softmax(y_pred, axis=-1)\n","\n","\n","\n","    # Calcola intersezione e unione\n","\n","    intersection = tf.reduce_sum(y_true_one_hot * y_pred, axis=[0, 1, 2])\n","\n","    union = tf.reduce_sum(y_true_one_hot, axis=[0, 1, 2]) + tf.reduce_sum(y_pred, axis=[0, 1, 2])\n","\n","\n","\n","    dice = (2. * intersection + smooth) / (union + smooth)\n","\n","\n","\n","    # Restituisci 1 - Dice score per ottenere la loss\n","\n","    return 1 - tf.reduce_mean(dice)\n","\n","\n","\n","def combined_loss(y_true, y_pred):\n","\n","    alpha = 0.8#0.8\n","\n","    beta = 0.2#0.2\n","\n","    return alpha * dice_loss(y_true, y_pred) + beta * tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n"],"metadata":{"id":"M0a1ax5GHp4W","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:45:59.572411Z","iopub.execute_input":"2024-12-05T11:45:59.572653Z","iopub.status.idle":"2024-12-05T11:45:59.583474Z","shell.execute_reply.started":"2024-12-05T11:45:59.572622Z","shell.execute_reply":"2024-12-05T11:45:59.582724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":["list(class_weights.values())"],"metadata":{"id":"YK5Wkn2qHSky"},"id":"YK5Wkn2qHSky","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","def focal_loss(y_true, y_pred, gamma=2.0, alpha=list(class_weights.values())):\n","    y_true = tf.cast(y_true, tf.int32)\n","    y_pred = tf.clip_by_value(y_pred, 1e-6, 1 - 1e-6)  # Evita log(0)\n","    y_true_one_hot = tf.one_hot(y_true, depth=y_pred.shape[-1])\n","\n","    # Pesi delle classi\n","    alpha_tensor = tf.constant(alpha, dtype=tf.float32)\n","\n","    # Calcola la Focal Loss\n","    weight = tf.gather(alpha_tensor, y_true)\n","    focal_loss = -weight * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred)\n","    return tf.reduce_mean(tf.reduce_sum(focal_loss, axis=-1))\n","\n","def combined_loss_FocalCross(y_true, y_pred, alpha=0.5, beta=0.5, gamma=2.0, focal_alpha=0.25):\n","    \"\"\"\n","    Combina Focal Loss e Sparse Categorical Crossentropy.\n","\n","    Args:\n","        y_true: Ground truth (valori reali).\n","        y_pred: Predizioni del modello.\n","        alpha: Peso della Focal Loss.\n","        beta: Peso della Sparse Categorical Crossentropy.\n","        gamma: Fattore gamma per la Focal Loss.\n","        focal_alpha: Peso alpha per la Focal Loss.\n","\n","    Returns:\n","        Combined loss scalare.\n","    \"\"\"\n","    focal = focal_loss(y_true, y_pred, gamma=gamma, alpha=focal_alpha)\n","    sparse_ce = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n","    return alpha * focal + beta * sparse_ce\n"],"metadata":{"id":"_z5iJIHaAOu9"},"id":"_z5iJIHaAOu9","execution_count":null,"outputs":[]},{"id":"R5AHdmibOWHe","cell_type":"code","source":["import tensorflow as tf\n","\n","\n","\n","def weighted_sparse_categorical_crossentropy(class_weights):\n","\n","    \"\"\"\n","\n","    Crea una funzione di perdita ponderata per la segmentazione semantica.\n","\n","\n","\n","    Args:\n","\n","        class_weights (dict): Dizionario con i pesi delle classi {classe: peso}.\n","\n","\n","\n","    Returns:\n","\n","        loss_fn: Funzione di perdita personalizzata.\n","\n","    \"\"\"\n","\n","    def loss_fn(y_true, y_pred):\n","\n","        # Assicurati che y_true sia int (necessario per tf.gather)\n","\n","        y_true = tf.cast(y_true, tf.int32)\n","\n","\n","\n","        # Calcola i pesi per ciascun pixel in y_true\n","\n","        weights = tf.gather(tf.constant(list(class_weights.values()), dtype=tf.float32), y_true)\n","\n","\n","\n","        # Rimuovi dimensioni extra da weights\n","\n","        weights = tf.squeeze(weights, axis=-1)  # Rimuove l'ultimo asse, se presente\n","\n","\n","\n","        # Calcola la SparseCategoricalCrossentropy\n","\n","        scce = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n","\n","\n","\n","        # Moltiplica la perdita per i pesi dei pixel\n","\n","        weighted_loss = scce * weights\n","\n","\n","\n","        # Restituisci la perdita media ponderata\n","\n","        return tf.reduce_mean(weighted_loss)\n","\n","\n","\n","    return loss_fn\n"],"metadata":{"id":"R5AHdmibOWHe","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:45:59.584415Z","iopub.execute_input":"2024-12-05T11:45:59.584717Z","iopub.status.idle":"2024-12-05T11:45:59.596106Z","shell.execute_reply.started":"2024-12-05T11:45:59.584692Z","shell.execute_reply":"2024-12-05T11:45:59.595391Z"}},"outputs":[],"execution_count":null},{"id":"tSlLo9Mx1dPr","cell_type":"code","source":["# Compile the model\n","\n","\n","\n","\n","\n","\n","\n","print(\"Compiling model...\")\n","\n","\n","\n","model.compile(\n","\n","\n","\n","    loss= weighted_sparse_categorical_crossentropy(class_weights),#tf.keras.losses.SparseCategoricalCrossentropy()\n","\n","\n","\n","    optimizer=tf.keras.optimizers.AdamW(LEARNING_RATE,weight_decay=1e-4),\n","\n","\n","\n","    metrics=[\"accuracy\", MeanIntersectionOverUnion(num_classes=NUM_CLASSES, labels_to_exclude=[0]), IoUPerClass(num_classes=NUM_CLASSES)]\n","\n","\n","\n",")\n","\n","\n","\n","print(\"Model compiled!\")"],"metadata":{"id":"tSlLo9Mx1dPr","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:45:59.597580Z","iopub.execute_input":"2024-12-05T11:45:59.597825Z","iopub.status.idle":"2024-12-05T11:45:59.622707Z","shell.execute_reply.started":"2024-12-05T11:45:59.597801Z","shell.execute_reply":"2024-12-05T11:45:59.621870Z"}},"outputs":[],"execution_count":null},{"id":"H6J65MMp4pA8","cell_type":"code","source":["# Setup callbacks\n","\n","\n","\n","early_stopping = tf.keras.callbacks.EarlyStopping(\n","\n","\n","\n","    monitor='val_mean_iou',\n","\n","\n","\n","    mode='max',\n","\n","\n","\n","    patience=PATIENCE,\n","\n","\n","\n","    restore_best_weights=True\n","\n","\n","\n",")\n","\n","\n","\n","\n","\n","\n","\n","viz_callback = VizCallback(val_img[0], val_lbl[0])\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n","\n","    monitor='val_mean_iou',  # Puoi cambiare con 'val_mean_io_u' se usi IoU\n","\n","    mode=\"max\",\n","\n","    factor=0.1,          # Riduci il learning rate di 0.2, prima era di 0.5\n","\n","    patience=10,          # Aspetta 5 epoche senza miglioramenti\n","\n","    min_delta=0.001,\n","\n","    min_lr=1e-5,         # Non scendere sotto 1e-6\n","\n","    verbose=1            # Mostra messaggi quando il learning rate cambia\n","\n","\n","\n",")\n"],"metadata":{"id":"H6J65MMp4pA8","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:45:59.644395Z","iopub.execute_input":"2024-12-05T11:45:59.645106Z","iopub.status.idle":"2024-12-05T11:45:59.650138Z","shell.execute_reply.started":"2024-12-05T11:45:59.645062Z","shell.execute_reply":"2024-12-05T11:45:59.649296Z"}},"outputs":[],"execution_count":null},{"id":"QFBHmdh2Xiny","cell_type":"code","source":["class IoUCallback(tf.keras.callbacks.Callback):\n","\n","    def __init__(self, frequency=5, **kwargs):\n","\n","        super(IoUCallback, self).__init__(**kwargs)  # Non passiamo 'name' al costruttore di Callback\n","\n","        self.frequency = frequency  # Numero di epoche tra i calcoli\n","\n","\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","\n","        # Verifica che l'epoca sia un multiplo di \"frequency\"\n","\n","        if (epoch + 1) % self.frequency == 0:\n","\n","            logs = logs or {}  # Assicura che logs sia un dizionario\n","\n","\n","\n","            # Ottieni l'IoU per classe utilizzando il metodo result()\n","\n","            iou_per_class = logs.get('iou_per_class')\n","\n","\n","\n","            # Verifica se iou_per_class Ã¨ disponibile e non None\n","\n","            if iou_per_class is not None and tf.is_tensor(iou_per_class) and iou_per_class.shape.ndims > 0:\n","\n","                for i, iou in enumerate(iou_per_class.numpy()):  # Converte in numpy per la stampa\n","\n","                    print(f\"Epoch {epoch + 1}, IoU for class {i}: {iou:.4f}\")\n","\n","            else:\n","\n","                print(f\"Epoch {epoch + 1}, IoU per class not available or empty.\")\n"],"metadata":{"id":"QFBHmdh2Xiny","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:45:59.651973Z","iopub.execute_input":"2024-12-05T11:45:59.652848Z","iopub.status.idle":"2024-12-05T11:45:59.663895Z","shell.execute_reply.started":"2024-12-05T11:45:59.652807Z","shell.execute_reply":"2024-12-05T11:45:59.663116Z"}},"outputs":[],"execution_count":null},{"id":"oXLduy0m1dNQ","cell_type":"code","source":["# Train the model\n","\n","\n","\n","history = model.fit(\n","\n","\n","\n","    train_dataset,\n","\n","\n","\n","    epochs=EPOCHS,\n","\n","\n","\n","    validation_data=val_dataset,\n","\n","\n","\n","    callbacks=[early_stopping, viz_callback,reduce_lr, IoUCallback(frequency=5)],#reduce_lr\n","\n","\n","\n","    verbose=1\n","\n","\n","\n",").history\n","\n","\n","\n","\n","\n","\n","\n","# Calculate and print the final validation accuracy\n","\n","\n","\n","final_val_meanIoU = round(max(history['val_mean_iou'])* 100, 2)\n","\n","\n","\n","print(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}%')\n","\n","\n","\n","\n","\n","\n","\n","# Save the trained model to a file with the accuracy included in the filename\n","\n","\n","\n","model_filename = 'Unet_'+str(final_val_meanIoU)+'.keras'\n","\n","\n","\n","model.save(model_filename)\n","\n","\n","\n","\n","\n","\n","\n","# Delete the model to free up resources\n","\n","\n","\n","del model"],"metadata":{"id":"oXLduy0m1dNQ","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T11:45:59.664820Z","iopub.execute_input":"2024-12-05T11:45:59.665080Z","iopub.status.idle":"2024-12-05T12:00:15.053097Z","shell.execute_reply.started":"2024-12-05T11:45:59.665044Z","shell.execute_reply":"2024-12-05T12:00:15.052146Z"}},"outputs":[],"execution_count":null},{"id":"wBxIwurlyv6N","cell_type":"code","source":["# Plot and display training and validation loss\n","\n","\n","\n","plt.figure(figsize=(18, 3))\n","\n","\n","\n","plt.plot(history['loss'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n","\n","\n","\n","plt.plot(history['val_loss'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n","\n","\n","\n","plt.title('Cross Entropy')\n","\n","\n","\n","plt.legend()\n","\n","\n","\n","plt.grid(alpha=0.3)\n","\n","\n","\n","plt.show()\n","\n","\n","\n","\n","\n","\n","\n","# Plot and display training and validation accuracy\n","\n","\n","\n","plt.figure(figsize=(18, 3))\n","\n","\n","\n","plt.plot(history['accuracy'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n","\n","\n","\n","plt.plot(history['val_accuracy'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n","\n","\n","\n","plt.title('Accuracy')\n","\n","\n","\n","plt.legend()\n","\n","\n","\n","plt.grid(alpha=0.3)\n","\n","\n","\n","plt.show()\n","\n","\n","\n","\n","\n","\n","\n","# Plot and display training and validation mean IoU\n","\n","\n","\n","plt.figure(figsize=(18, 3))\n","\n","\n","\n","plt.plot(history['mean_iou'], label='Training', alpha=0.8, color='#ff7f0e', linewidth=2)\n","\n","\n","\n","plt.plot(history['val_mean_iou'], label='Validation', alpha=0.9, color='#5a9aa5', linewidth=2)\n","\n","\n","\n","plt.title('Mean Intersection over Union')\n","\n","\n","\n","plt.legend()\n","\n","\n","\n","plt.grid(alpha=0.3)\n","\n","\n","\n","plt.show()"],"metadata":{"id":"wBxIwurlyv6N","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:00:15.054151Z","iopub.execute_input":"2024-12-05T12:00:15.054449Z","iopub.status.idle":"2024-12-05T12:00:15.926798Z","shell.execute_reply.started":"2024-12-05T12:00:15.054422Z","shell.execute_reply":"2024-12-05T12:00:15.925941Z"}},"outputs":[],"execution_count":null},{"id":"PVvA415Ifll8","cell_type":"markdown","source":["## ðŸ•¹ï¸ Use the Model - Make Inference"],"metadata":{"id":"PVvA415Ifll8"}},{"id":"-j7J_CT9yv3o","cell_type":"code","source":["# Load UNet model without compiling\n","\n","\n","\n","model = tfk.models.load_model(model_filename, compile=False)\n","\n","\n","\n","\n","\n","\n","\n","# Compile the model with specified loss, optimizer, and metrics\n","\n","\n","\n","model.compile(\n","\n","\n","\n","    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","\n","\n","\n","    optimizer=tfk.optimizers.AdamW(LEARNING_RATE),\n","\n","\n","\n","    metrics=[\"accuracy\", MeanIntersectionOverUnion(num_classes=NUM_CLASSES, labels_to_exclude=[0])]\n","\n","\n","\n",")\n","\n","\n","\n","\n","\n","\n","\n","# Print a detailed summary of the model with expanded nested layers and trainable parameters.\n","\n","\n","\n","model.summary(expand_nested=True, show_trainable=True)\n","\n","\n","\n","\n","\n","\n","\n","# Generate and display a graphical representation of the model architecture.\n","\n","\n","\n","#tf.keras.utils.plot_model(model, show_trainable=True, expand_nested=True, dpi=70)"],"metadata":{"id":"-j7J_CT9yv3o","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:00:15.929502Z","iopub.execute_input":"2024-12-05T12:00:15.929835Z","iopub.status.idle":"2024-12-05T12:00:16.411949Z","shell.execute_reply.started":"2024-12-05T12:00:15.929808Z","shell.execute_reply":"2024-12-05T12:00:16.411064Z"}},"outputs":[],"execution_count":null},{"id":"RZh4Ta8Oj3t_","cell_type":"code","source":["def visualize_colormap(colormap, num_classes):\n","\n","    \"\"\"\n","\n","    Visualize the given colormap by plotting color swatches with class indices.\n","\n","\n","\n","    Parameters:\n","\n","    colormap: numpy.ndarray - The colormap array with shape (num_classes, 3 or 4).\n","\n","    num_classes: int - The number of classes.\n","\n","    \"\"\"\n","\n","    plt.figure(figsize=(10, 2))\n","\n","\n","\n","    # Create a horizontal bar for each class\n","\n","    for i in range(num_classes):\n","\n","        plt.bar(i, 1, color=colormap[i], edgecolor=\"black\")\n","\n","\n","\n","    # Add labels for each class\n","\n","    plt.xticks(range(num_classes), labels=range(num_classes), fontsize=12)\n","\n","    plt.yticks([])  # Hide y-axis\n","\n","    plt.title(\"Colormap Visualization\", fontsize=14)\n","\n","    plt.xlabel(\"Class Index\", fontsize=12)\n","\n","    plt.tight_layout()\n","\n","    plt.show()\n","\n","\n","\n","visualize_colormap(COLORMAP, NUM_CLASSES)"],"metadata":{"id":"RZh4Ta8Oj3t_","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:00:16.413252Z","iopub.execute_input":"2024-12-05T12:00:16.413922Z","iopub.status.idle":"2024-12-05T12:00:16.624185Z","shell.execute_reply.started":"2024-12-05T12:00:16.413880Z","shell.execute_reply":"2024-12-05T12:00:16.622935Z"}},"outputs":[],"execution_count":null},{"id":"jNjpbWmGwbR9","cell_type":"code","source":["def plot_triptychs(dataset, model, num_samples=1):\n","\n","\n","\n","    \"\"\"\n","\n","\n","\n","    Plot triptychs (original image, true mask, predicted mask) for samples from a tf.data.Dataset\n","\n","\n","\n","\n","\n","\n","\n","    Parameters:\n","\n","\n","\n","    dataset: tf.data.Dataset - The dataset containing image-label pairs\n","\n","\n","\n","    model: tf.keras.Model - The trained model to generate predictions\n","\n","\n","\n","    num_samples: int - Number of samples to plot\n","\n","\n","\n","    \"\"\"\n","\n","\n","\n","    # Take samples from the dataset\n","\n","\n","\n","    # Unbatch the dataset to get individual samples\n","\n","    unbatched_dataset = dataset.unbatch().take(num_samples)\n","\n","\n","\n","\n","\n","\n","\n","    for images, labels in unbatched_dataset.as_numpy_iterator():\n","\n","\n","\n","\n","\n","      # Ensure images and labels are expanded to simulate a batch\n","\n","        images = np.expand_dims(images, axis=0)\n","\n","        labels = np.expand_dims(labels, axis=0)\n","\n","\n","\n","\n","\n","        # Generate predictions\n","\n","\n","\n","        pred = model.predict(images, verbose=0)\n","\n","\n","\n","        pred = tf.math.argmax(pred, axis=-1)\n","\n","\n","\n","\n","\n","\n","\n","        # Create colormap based on number of classes in labels\n","\n","\n","\n","        num_classes = len(np.unique(labels))\n","\n","\n","\n","        colormap = create_segmentation_colormap(num_classes)\n","\n","\n","\n","\n","\n","\n","\n","        # Create figure with subplots\n","\n","\n","\n","        fig, axes = plt.subplots(1, 3, figsize=(20, 4))\n","\n","\n","\n","\n","\n","\n","\n","        # Plot original image\n","\n","\n","\n","        axes[0].set_title(\"Original Image\")\n","\n","\n","\n","        axes[0].imshow(images[0])\n","\n","\n","\n","        axes[0].axis('off')\n","\n","\n","\n","\n","\n","\n","\n","        # Plot original mask\n","\n","\n","\n","        axes[1].set_title(\"Original Mask\")\n","\n","\n","\n","        colored_label = apply_colormap(np.squeeze(labels[0], axis=-1), colormap)\n","\n","\n","\n","        axes[1].imshow(colored_label)\n","\n","\n","\n","        axes[1].axis('off')\n","\n","\n","\n","\n","\n","\n","\n","        # Plot predicted mask\n","\n","\n","\n","        axes[2].set_title(\"Predicted Mask\")\n","\n","\n","\n","        colored_pred = apply_colormap(pred[0], colormap)\n","\n","\n","\n","        axes[2].imshow(colored_pred)\n","\n","\n","\n","        axes[2].axis('off')\n","\n","\n","\n","\n","\n","\n","\n","        plt.tight_layout()\n","\n","\n","\n","        plt.show()\n","\n","\n","\n","        plt.close()\n","\n","\n","\n","\n","\n","\n","\n","# Example usage:\n","\n","\n","\n","# Plot three random samples\n","\n","\n","\n","plot_triptychs(val_dataset, model, num_samples=5)"],"metadata":{"id":"jNjpbWmGwbR9","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:00:16.626069Z","iopub.execute_input":"2024-12-05T12:00:16.626599Z","iopub.status.idle":"2024-12-05T12:00:19.955712Z","shell.execute_reply.started":"2024-12-05T12:00:16.626540Z","shell.execute_reply":"2024-12-05T12:00:19.954706Z"}},"outputs":[],"execution_count":null},{"id":"lumV_jhmRNpO","cell_type":"code","source":["#timestep_str = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n","\n","#model_filename = f\"model_{timestep_str}.keras\"\n","\n","model.save(model_filename)\n","\n","del model\n","\n","\n","\n","print(f\"Model saved to {model_filename}\")"],"metadata":{"id":"lumV_jhmRNpO","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:00:19.956852Z","iopub.execute_input":"2024-12-05T12:00:19.957133Z","iopub.status.idle":"2024-12-05T12:00:20.038502Z","shell.execute_reply.started":"2024-12-05T12:00:19.957106Z","shell.execute_reply":"2024-12-05T12:00:20.037647Z"}},"outputs":[],"execution_count":null},{"id":"0abe5b72","cell_type":"markdown","source":["## ðŸ“Š Prepare Your Submission\n","\n","\n","\n","In our Kaggle competition, submissions are made as `csv` files. To create a proper `csv` file, you need to flatten your predictions and include an `id` column as the first column of your dataframe. To maintain consistency between your results and our solution, please avoid shuffling the test set. The code below demonstrates how to prepare the `csv` file from your model predictions."],"metadata":{"id":"0abe5b72"}},{"id":"D1wLXrkDROrJ","cell_type":"code","source":["# If model_filename is not defined, load the most recent model from Google Drive\n","\n","if \"model_filename\" not in globals() or model_filename is None:\n","\n","    files = [f for f in os.listdir('.') if os.path.isfile(f) and f.startswith('model_') and f.endswith('.keras')]\n","\n","    files.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n","\n","    if files:\n","\n","        model_filename = files[0]\n","\n","    else:\n","\n","        raise FileNotFoundError(\"No model files found in the current directory.\")"],"metadata":{"id":"D1wLXrkDROrJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:00:20.039513Z","iopub.execute_input":"2024-12-05T12:00:20.039775Z","iopub.status.idle":"2024-12-05T12:00:20.044759Z","shell.execute_reply.started":"2024-12-05T12:00:20.039750Z","shell.execute_reply":"2024-12-05T12:00:20.043899Z"}},"outputs":[],"execution_count":null},{"id":"Ogcmt-HKRU7B","cell_type":"code","source":["model = tfk.models.load_model(model_filename, compile=False)\n","\n","print(f\"Model loaded from {model_filename}\")"],"metadata":{"id":"Ogcmt-HKRU7B","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:00:20.045905Z","iopub.execute_input":"2024-12-05T12:00:20.046562Z","iopub.status.idle":"2024-12-05T12:00:20.296775Z","shell.execute_reply.started":"2024-12-05T12:00:20.046523Z","shell.execute_reply":"2024-12-05T12:00:20.295895Z"}},"outputs":[],"execution_count":null},{"id":"m0x435dRRXhI","cell_type":"code","source":["preds = model.predict(X_test)\n","\n","preds = np.argmax(preds, axis=-1)\n","\n","print(f\"Predictions shape: {preds.shape}\")"],"metadata":{"id":"m0x435dRRXhI","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:00:20.297852Z","iopub.execute_input":"2024-12-05T12:00:20.298115Z","iopub.status.idle":"2024-12-05T12:00:31.971672Z","shell.execute_reply.started":"2024-12-05T12:00:20.298089Z","shell.execute_reply":"2024-12-05T12:00:31.970787Z"}},"outputs":[],"execution_count":null},{"id":"0m1IW1cvRZBY","cell_type":"code","source":["def y_to_df(y) -> pd.DataFrame:\n","\n","    \"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n","\n","    n_samples = len(y)\n","\n","    y_flat = y.reshape(n_samples, -1)\n","\n","    df = pd.DataFrame(y_flat)\n","\n","    df[\"id\"] = np.arange(n_samples)\n","\n","    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n","\n","    return df[cols]"],"metadata":{"id":"0m1IW1cvRZBY","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:00:31.972886Z","iopub.execute_input":"2024-12-05T12:00:31.973597Z","iopub.status.idle":"2024-12-05T12:00:31.978805Z","shell.execute_reply.started":"2024-12-05T12:00:31.973551Z","shell.execute_reply":"2024-12-05T12:00:31.977791Z"}},"outputs":[],"execution_count":null},{"id":"DbcECRgURarI","cell_type":"code","source":["# Create and download the csv submission file\n","\n","timestep_str = model_filename.replace(\"model_\", \"\").replace(\".keras\", \"\")\n","\n","os.makedirs(\"submissions\", exist_ok=True)\n","\n","submission_filename = f\"submissions/submission_{timestep_str}.csv\"\n","\n","submission_df = y_to_df(preds)\n","\n","submission_df.to_csv(submission_filename, index=False)\n","\n","\n","\n","if IN_COLAB:\n","\n","    from google.colab import files\n","\n","    files.download(submission_filename)"],"metadata":{"id":"DbcECRgURarI","trusted":true,"execution":{"iopub.status.busy":"2024-12-05T12:00:31.979764Z","iopub.execute_input":"2024-12-05T12:00:31.980038Z","iopub.status.idle":"2024-12-05T12:00:53.797952Z","shell.execute_reply.started":"2024-12-05T12:00:31.980013Z","shell.execute_reply":"2024-12-05T12:00:53.797092Z"}},"outputs":[],"execution_count":null}]}